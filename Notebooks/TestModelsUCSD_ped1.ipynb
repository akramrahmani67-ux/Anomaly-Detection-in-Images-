{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "\n",
    "# # Mount Google Drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Define the dataset storage path in Google Drive\n",
    "# dataset_path = \"/content/drive/MyDrive/UCSD_Anomaly_Dataset.tar.gz\"\n",
    "# extract_path = \"/content/drive/MyDrive/UCSD_Anomaly_Dataset/\"\n",
    "\n",
    "# # Download the dataset (only if it does not already exist)\n",
    "# if not os.path.exists(dataset_path):\n",
    "#     !wget -c \"http://www.svcl.ucsd.edu/projects/anomaly/UCSD_Anomaly_Dataset.tar.gz\" -O \"{dataset_path}\"\n",
    "\n",
    "# # Extract the dataset directly in Google Drive\n",
    "# !mkdir -p \"{extract_path}\"\n",
    "# !tar -xzf \"{dataset_path}\" -C \"{extract_path}\"\n",
    "\n",
    "# # Remove the compressed file to save space\n",
    "# !rm \"{dataset_path}\"\n",
    "\n",
    "# print(\"‚úÖ Dataset downloaded and saved to Google Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# folder_path = '/content/drive/MyDrive/UCSD_Anomaly_Dataset'\n",
    "\n",
    "# # ÿßÿ≥ÿ™ŸÅÿßÿØŸá ÿßÿ≤ ÿØÿ≥ÿ™Ÿàÿ± ÿ≥€åÿ≥ÿ™ŸÖ ÿ®ÿ±ÿß€å ÿ≠ÿ∞ŸÅ ÿ≥ÿ±€åÿπ‚Äåÿ™ÿ±\n",
    "# os.system(f'rm -rf \"{folder_path}\"')\n",
    "\n",
    "# print(\"‚úÖ Folder and its contents have been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -c \"http://www.svcl.ucsd.edu/projects/anomaly/UCSD_Anomaly_Dataset.tar.gz\" -O \"/content/drive/MyDrive/UCSD_Anomaly_Dataset.tar.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from google.colab import drive\n",
    "\n",
    "# ‚úÖ Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ‚úÖ Dataset path\n",
    "DATASET_PATH = \"/content/drive/MyDrive/UCSD_Anomaly_Dataset/UCSD_Anomaly_Dataset.v1p2/UCSDped1\"\n",
    "\n",
    "# ‚úÖ Anomalous frame ranges for each test video\n",
    "TestVideoFile = {\n",
    "    \"Test001\": range(59,152), \"Test002\": range(49,175), \"Test003\": range(90,200), \"Test004\": range(30,168),\n",
    "    \"Test005\": list(range(4,90)) + list(range(139,200)), \"Test006\": list(range(0,100)) + list(range(109,200)),\n",
    "    \"Test007\": range(0,175), \"Test008\": range(0,94), \"Test009\": range(0,48), \"Test010\": range(0,140),\n",
    "    \"Test011\": range(69,165), \"Test012\": range(130,200), \"Test013\": range(0,156), \"Test014\": range(0,200),\n",
    "    \"Test015\": range(137,200), \"Test016\": range(122,200), \"Test017\": range(0,47), \"Test018\": range(53,120),\n",
    "    \"Test019\": range(63,138), \"Test020\": range(44,175), \"Test021\": range(30,200), \"Test022\": range(16,107),\n",
    "    \"Test023\": range(8,165), \"Test024\": range(49,171), \"Test025\": range(39,135), \"Test026\": range(77,144),\n",
    "    \"Test027\": range(9,122), \"Test028\": range(104,200), \"Test029\": list(range(0,15)) + list(range(44,113)),\n",
    "    \"Test030\": range(174,200), \"Test031\": range(0,180), \"Test032\": list(range(0,52)) + list(range(64,115)),\n",
    "    \"Test033\": range(4,165), \"Test034\": range(0,121), \"Test035\": range(85,200), \"Test036\": range(14,108)\n",
    "}\n",
    "\n",
    "def count_frames(dataset_type):\n",
    "    path = os.path.join(DATASET_PATH, dataset_type)\n",
    "    normal_count = 0\n",
    "    anomaly_count = 0\n",
    "\n",
    "    for folder in sorted(os.listdir(path)):\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        if os.path.isdir(folder_path) and not folder.endswith(\"_gt\"):\n",
    "            for filename in sorted(os.listdir(folder_path)):\n",
    "                if filename.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp','tif')):\n",
    "                    try:\n",
    "                        frame_idx = int(os.path.splitext(filename)[0])\n",
    "                        if dataset_type == \"Test\":\n",
    "                            anomaly_range = TestVideoFile.get(folder, [])\n",
    "                            if frame_idx in anomaly_range:\n",
    "                                anomaly_count += 1\n",
    "                            else:\n",
    "                                normal_count += 1\n",
    "                        else:\n",
    "                            normal_count += 1\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "    return normal_count, anomaly_count\n",
    "\n",
    "# ‚úÖ Count for both Train and Test\n",
    "train_normal, train_abnormal = count_frames(\"Train\")\n",
    "test_normal, test_abnormal = count_frames(\"Test\")\n",
    "\n",
    "# ‚úÖ Display results\n",
    "print(f\"Train Set ‚Üí Normal: {train_normal}, Anomalous: {train_abnormal} (should be 0)\")\n",
    "print(f\"Test Set  ‚Üí Normal: {test_normal}, Anomalous: {test_abnormal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "# ‚úÖ Paths for Train and Test directories\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, \"Train\")\n",
    "TEST_PATH = os.path.join(DATASET_PATH, \"Test\")\n",
    "\n",
    "# ‚úÖ Output directories for extracted frames\n",
    "NORMAL_DIR = \"/content/All_Normal_Frames\"\n",
    "ANOMALY_DIR = \"/content/All_Anomaly_Frames\"\n",
    "\n",
    "# ‚úÖ Create output directories if they don't exist\n",
    "os.makedirs(NORMAL_DIR, exist_ok=True)\n",
    "os.makedirs(ANOMALY_DIR, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Frame counters\n",
    "normal_count = 0\n",
    "anomaly_count = 0\n",
    "\n",
    "# ‚úÖ Function to process folders and extract frames\n",
    "def process_directory(directory_path, is_test=False):\n",
    "    global normal_count, anomaly_count\n",
    "\n",
    "    for folder in sorted(os.listdir(directory_path)):\n",
    "        folder_path = os.path.join(directory_path, folder)\n",
    "\n",
    "        if os.path.isdir(folder_path) and not folder.endswith(\"_gt\"):\n",
    "            anomaly_range = TestVideoFile.get(folder, []) if is_test else []\n",
    "\n",
    "            for filename in sorted(os.listdir(folder_path)):\n",
    "                if filename.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp', 'tif', 'tiff')):\n",
    "                    try:\n",
    "                        frame_idx = int(os.path.splitext(filename)[0])\n",
    "                        src_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                        if is_test and frame_idx in anomaly_range:\n",
    "                            dst_path = os.path.join(ANOMALY_DIR, f\"{folder}_{filename}\")\n",
    "                            shutil.copyfile(src_path, dst_path)\n",
    "                            anomaly_count += 1\n",
    "                        else:\n",
    "                            dst_path = os.path.join(NORMAL_DIR, f\"{folder}_{filename}\")\n",
    "                            shutil.copyfile(src_path, dst_path)\n",
    "                            normal_count += 1\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "# ‚úÖ Run processing for both Train and Test sets\n",
    "print(\"üîç Processing Train set...\")\n",
    "process_directory(TRAIN_PATH, is_test=False)\n",
    "\n",
    "print(\"üîç Processing Test set...\")\n",
    "process_directory(TEST_PATH, is_test=True)\n",
    "\n",
    "# ‚úÖ Print summary\n",
    "print(\"\\nüìä Frame Summary:\")\n",
    "print(f\"‚úÖ Total Normal Frames   : {normal_count}\")\n",
    "print(f\"‚úÖ Total Anomalous Frames: {anomaly_count}\")\n",
    "print(f\"\\nüìÅ Saved in:\\n  - Normal : {NORMAL_DIR}\\n  - Anomaly: {ANOMALY_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "NORMAL_DIR = \"/content/All_Normal_Frames\"\n",
    "ANOMALY_DIR_ORIGINAL = \"/content/All_Anomaly_Frames\"\n",
    "ANOMALY_DIR_BALANCED = \"/content/All_Anomaly_Frames_Balanced\"\n",
    "\n",
    "# Delete the previously balanced anomaly directory if it exists\n",
    "if os.path.exists(ANOMALY_DIR_BALANCED):\n",
    "    shutil.rmtree(ANOMALY_DIR_BALANCED)\n",
    "    print(f\"üóëÔ∏è Directory {ANOMALY_DIR_BALANCED} removed.\")\n",
    "\n",
    "# Create a new balanced anomaly directory\n",
    "os.makedirs(ANOMALY_DIR_BALANCED, exist_ok=True)\n",
    "print(f\"üìÅ New directory {ANOMALY_DIR_BALANCED} created.\")\n",
    "\n",
    "# Only consider .tif files\n",
    "original_files = sorted([f for f in os.listdir(ANOMALY_DIR_ORIGINAL) if f.lower().endswith('.tif')])\n",
    "\n",
    "# Copy original anomaly files\n",
    "for fname in original_files:\n",
    "    src = os.path.join(ANOMALY_DIR_ORIGINAL, fname)\n",
    "    dst = os.path.join(ANOMALY_DIR_BALANCED, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Target anomaly frame count equal to number of normal frames\n",
    "TARGET_ANOMALY_COUNT = len([f for f in os.listdir(NORMAL_DIR) if f.lower().endswith('.tif')])\n",
    "\n",
    "print(f\"üéØ Target: {TARGET_ANOMALY_COUNT} anomaly frames\")\n",
    "\n",
    "augmented_count = len(original_files)\n",
    "idx = 0\n",
    "\n",
    "while augmented_count < TARGET_ANOMALY_COUNT and idx < len(original_files):\n",
    "    img_path = os.path.join(ANOMALY_DIR_ORIGINAL, original_files[idx])\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"‚ö†Ô∏è Corrupted file skipped: {img_path}\")\n",
    "        idx += 1\n",
    "        continue\n",
    "\n",
    "    flipped = cv2.flip(img, 1)\n",
    "    rotated90 = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    rotated180 = cv2.rotate(img, cv2.ROTATE_180)\n",
    "\n",
    "    base_name = os.path.splitext(original_files[idx])[0]\n",
    "\n",
    "    for aug_img, suffix in zip([flipped, rotated90, rotated180], ['_flip', '_r90', '_r180']):\n",
    "        if augmented_count >= TARGET_ANOMALY_COUNT:\n",
    "            break\n",
    "        new_name = f\"{base_name}{suffix}.tif\"\n",
    "        save_path = os.path.join(ANOMALY_DIR_BALANCED, new_name)\n",
    "        cv2.imwrite(save_path, aug_img)\n",
    "        augmented_count += 1\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "# Final counts of .tif files in each directory\n",
    "final_normal = len([f for f in os.listdir(NORMAL_DIR) if f.lower().endswith('.tif')])\n",
    "final_anomaly = len([f for f in os.listdir(ANOMALY_DIR_BALANCED) if f.lower().endswith('.tif')])\n",
    "\n",
    "print(\"\\nüìä Final Frame Count Summary:\")\n",
    "print(f\"‚úÖ Normal frames: {final_normal}\")\n",
    "print(f\"‚úÖ Anomaly frames (balanced): {final_anomaly}\")\n",
    "print(f\"üìÅ Normal frames path: {NORMAL_DIR}\")\n",
    "print(f\"üìÅ Anomaly frames path: {ANOMALY_DIR_BALANCED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Main data directories\n",
    "NORMAL_DIR = \"/content/All_Normal_Frames\"\n",
    "ANOMALY_DIR = \"/content/All_Anomaly_Frames_Balanced\"  # or any path where anomaly frames are balanced\n",
    "\n",
    "# Destination directory for Keras-compatible structure\n",
    "BALANCED_DIR = \"/content/frames_balanced\"\n",
    "BALANCED_NORMAL = os.path.join(BALANCED_DIR, \"normal\")\n",
    "BALANCED_ANOMALY = os.path.join(BALANCED_DIR, \"abnormal\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(BALANCED_NORMAL, exist_ok=True)\n",
    "os.makedirs(BALANCED_ANOMALY, exist_ok=True)\n",
    "\n",
    "# Copy normal frames\n",
    "for fname in os.listdir(NORMAL_DIR):\n",
    "    src = os.path.join(NORMAL_DIR, fname)\n",
    "    dst = os.path.join(BALANCED_NORMAL, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy anomaly frames\n",
    "for fname in os.listdir(ANOMALY_DIR):\n",
    "    src = os.path.join(ANOMALY_DIR, fname)\n",
    "    dst = os.path.join(BALANCED_ANOMALY, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "print(f\"‚úÖ Folders created and files copied to:\\n{BALANCED_NORMAL}\\n{BALANCED_ANOMALY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "# ‚öôÔ∏è Settings\n",
    "IMG_SIZE = 224\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# üìÅ Data paths\n",
    "normal_dir = \"/content/frames_balanced/normal\"\n",
    "anomaly_dir = \"/content/frames_balanced/abnormal\"\n",
    "save_dir = \"/content\"\n",
    "\n",
    "# ‚úÖ List of models\n",
    "model_names = [\n",
    "    'convnext_tiny',\n",
    "    'repvgg_a0',\n",
    "    'mobileone_s0',\n",
    "    'poolformer_s12',\n",
    "    'maxvit_tiny_tf_224',\n",
    "    'coatnet_0_rw_224'\n",
    "]\n",
    "\n",
    "# ‚úÖ Image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# üì• Get list of image paths\n",
    "def get_image_paths(directory):\n",
    "    return [os.path.join(directory, f)\n",
    "            for f in os.listdir(directory)\n",
    "            if f.lower().endswith(('.jpg', '.jpeg', '.png','tif','tiff'))]\n",
    "\n",
    "# üîç Feature extraction\n",
    "def extract_features(model, image_paths):\n",
    "    features = []\n",
    "    for path in tqdm(image_paths, desc=\"Extracting features\"):\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat = model(img_tensor)\n",
    "        features.append(feat.cpu().numpy().squeeze())\n",
    "    return np.array(features)\n",
    "\n",
    "# üîÅ Run processing for each model\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nüîß Processing model: {model_name}\")\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "    model.eval()\n",
    "\n",
    "    # ‚õî Freeze 85% of layers\n",
    "    total_layers = sum(1 for _ in model.parameters())\n",
    "    trainable_start = int(total_layers * 0.85)\n",
    "    for i, param in enumerate(model.parameters()):\n",
    "        param.requires_grad = i >= trainable_start\n",
    "    model.to(device)\n",
    "\n",
    "    # üîπ Normal and anomaly frames\n",
    "    normal_paths = get_image_paths(normal_dir)\n",
    "    anomaly_paths = get_image_paths(anomaly_dir)\n",
    "\n",
    "    print(\"üîπ Extracting NORMAL features...\")\n",
    "    normal_features = extract_features(model, normal_paths)\n",
    "\n",
    "    print(\"üî∏ Extracting ANOMALY features...\")\n",
    "    anomaly_features = extract_features(model, anomaly_paths)\n",
    "\n",
    "    # üíæ Save features\n",
    "    model_id = model_name.replace('/', '_')\n",
    "    np.save(os.path.join(save_dir, f'normal_features_{model_id}.npy'), normal_features)\n",
    "    np.save(os.path.join(save_dir, f'anomaly_features_{model_id}.npy'), anomaly_features)\n",
    "\n",
    "    print(f\"‚úÖ Saved features for model: {model_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    model_id = model_name.replace('/', '_')\n",
    "    normal_path = os.path.join(save_dir, f'normal_features_{model_id}.npy')\n",
    "    anomaly_path = os.path.join(save_dir, f'anomaly_features_{model_id}.npy')\n",
    "\n",
    "    if not os.path.exists(normal_path) or not os.path.exists(anomaly_path):\n",
    "        print(f\"‚ùå Missing: {model_id}\")\n",
    "        continue\n",
    "\n",
    "    normal_features = np.load(normal_path)\n",
    "    anomaly_features = np.load(anomaly_path)\n",
    "\n",
    "    print(f\"‚úÖ {model_id} - Normal: {normal_features.shape}, Anomaly: {anomaly_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grad-cam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# ‚öôÔ∏è Path to abnormal images\n",
    "anomaly_dir = \"/content/All_Anomaly_Frames\"\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# üåÄ Preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# üß† Load model\n",
    "model = timm.create_model('repvgg_a0', pretrained=True, num_classes=2)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# üîç Grad-CAM++\n",
    "class GradCAMPlusPlus:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    def generate(self, input_tensor):\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_tensor)\n",
    "        class_idx = torch.argmax(output, dim=1).item()\n",
    "        score = output[0, class_idx]\n",
    "        score.backward()\n",
    "\n",
    "        gradients = self.gradients\n",
    "        activations = self.activations\n",
    "\n",
    "        b, k, h, w = gradients.size()\n",
    "        alpha_num = gradients.pow(2)\n",
    "        alpha_denom = 2 * gradients.pow(2) + activations * gradients.pow(3).sum(-1).sum(-1).view(b, k, 1, 1)\n",
    "        alpha_denom = torch.where(alpha_denom != 0.0, alpha_denom, torch.ones_like(alpha_denom))\n",
    "        alpha = alpha_num / alpha_denom\n",
    "\n",
    "        weights = torch.relu(F.relu(gradients) * alpha).sum(-1).sum(-1).view(b, k, 1, 1)\n",
    "        cam = (weights * activations).sum(1, keepdim=True)\n",
    "\n",
    "        cam = F.relu(cam)\n",
    "        cam = F.interpolate(cam, size=(IMG_SIZE, IMG_SIZE), mode='bilinear', align_corners=False)\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "        return cam.cpu().detach().numpy().squeeze()\n",
    "\n",
    "# üéØ Identify target convolutional layer\n",
    "conv_layers = [m for m in model.modules() if isinstance(m, torch.nn.Conv2d)]\n",
    "target_layer = conv_layers[-1]\n",
    "gradcam = GradCAMPlusPlus(model, target_layer)\n",
    "\n",
    "# üìÇ Get list of image paths\n",
    "all_image_paths = sorted([\n",
    "    os.path.join(anomaly_dir, fname)\n",
    "    for fname in os.listdir(anomaly_dir)\n",
    "    if fname.lower().endswith(('.jpg', '.jpeg', '.tif','tiff'))\n",
    "])\n",
    "\n",
    "# Randomly select 5 diverse images\n",
    "random.seed(42)  # for reproducibility\n",
    "image_paths = random.sample(all_image_paths, min(5, len(all_image_paths)))\n",
    "\n",
    "# üî• Process and display heatmaps\n",
    "for path in image_paths:\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        continue\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_tensor = preprocess(img_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate heatmap\n",
    "    heatmap = gradcam.generate(img_tensor)\n",
    "\n",
    "    # Convert to colored heatmap\n",
    "    heatmap_color = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize original image\n",
    "    img_resized = cv2.resize(img_rgb, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # Blend heatmap with original image\n",
    "    alpha = 0.4\n",
    "    blended = cv2.addWeighted(img_resized, 1 - alpha, heatmap_color, alpha, 0)\n",
    "\n",
    "    # Display with matplotlib + colorbar\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(blended)\n",
    "    plt.title(f\"Heatmap - {os.path.basename(path)}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Add colorbar from raw heatmap\n",
    "    im = plt.imshow(heatmap, cmap='jet', alpha=0)\n",
    "    cbar = plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Anomaly Score\", rotation=270, labelpad=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, GRU, SimpleRNN, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# --- Data loading and preprocessing ---\n",
    "def prepare_data(normal_path, anomaly_path):\n",
    "    X_normal = np.load(normal_path, allow_pickle=True)\n",
    "    if X_normal.ndim >= 4:\n",
    "        X_normal = X_normal.mean(axis=(2, 3))\n",
    "    X_anomaly = np.load(anomaly_path, allow_pickle=True)\n",
    "    if X_anomaly.ndim >= 4:\n",
    "        X_anomaly = X_anomaly.mean(axis=(2, 3))\n",
    "    X = np.concatenate([X_normal, X_anomaly])\n",
    "    y = np.concatenate([np.zeros(len(X_normal)), np.ones(len(X_anomaly))])\n",
    "    X = X.astype(np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    return X, y\n",
    "\n",
    "def create_sequences(X, y, seq_len=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len + 1):\n",
    "        X_seq.append(X[i:i + seq_len])\n",
    "        y_seq.append(y[i + seq_len - 1])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# --- Model builders with 32 and 16 units ---\n",
    "def build_lstm_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, activation='tanh', recurrent_activation='sigmoid', unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GRU(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_lstm_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, return_sequences=True, unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = GRU(32)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_simple_rnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = SimpleRNN(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_dense_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Main execution ---\n",
    "def main():\n",
    "    feature_model_name = 'convnext_tiny'  # Change if needed\n",
    "    seq_len = 10\n",
    "    save_dir = '/content/drive/MyDrive/ped1'  # Change to your data path\n",
    "    normal_path = f\"{save_dir}/normal_features_{feature_model_name}.npy\"\n",
    "    anomaly_path = f\"{save_dir}/anomaly_features_{feature_model_name}.npy\"\n",
    "\n",
    "    X, y = prepare_data(normal_path, anomaly_path)\n",
    "    X_seq, y_seq = create_sequences(X, y, seq_len=seq_len)\n",
    "\n",
    "    model_builders = {\n",
    "        \"LSTM\": build_lstm_model,\n",
    "        \"GRU\": build_gru_model,\n",
    "        \"LSTM_GRU\": build_lstm_gru_model,\n",
    "        \"SimpleRNN\": build_simple_rnn_model,\n",
    "        \"Dense\": build_dense_model\n",
    "    }\n",
    "\n",
    "    num_runs = 5\n",
    "    all_results = []\n",
    "\n",
    "    for model_name, builder in model_builders.items():\n",
    "        print(f\"\\n==== Training model: {model_name} ====\")\n",
    "\n",
    "        metrics_list = {\n",
    "            \"accuracy\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": [],\n",
    "            \"auc\": []\n",
    "        }\n",
    "\n",
    "        histories = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            print(f\" Run {run+1}/{num_runs}\")\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_seq, y_seq, test_size=0.2, stratify=y_seq, random_state=run\n",
    "            )\n",
    "\n",
    "            model = builder(X_train.shape[1:])\n",
    "            es = EarlyStopping(patience=5, restore_best_weights=True, verbose=0)\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[es],\n",
    "                verbose=1\n",
    "            )\n",
    "            histories.append(history)\n",
    "\n",
    "            y_prob = model.predict(X_test).ravel()\n",
    "            y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "            # Store metrics\n",
    "            metrics_list[\"accuracy\"].append(acc)\n",
    "            metrics_list[\"precision\"].append(prec)\n",
    "            metrics_list[\"recall\"].append(rec)\n",
    "            metrics_list[\"f1\"].append(f1)\n",
    "            metrics_list[\"auc\"].append(auc_score)\n",
    "\n",
    "            all_results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Run\": run + 1,\n",
    "                \"Accuracy\": acc,\n",
    "                \"Precision\": prec,\n",
    "                \"Recall\": rec,\n",
    "                \"F1\": f1,\n",
    "                \"AUC\": auc_score\n",
    "            })\n",
    "\n",
    "            # Clear to save memory\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "        # Plot training curves for last run\n",
    "        last_history = histories[-1]\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(last_history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(last_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'{model_name} Accuracy per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(last_history.history['loss'], label='Train Loss')\n",
    "        plt.plot(last_history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{model_name} Loss per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print mean ¬± std summary for all runs\n",
    "        print(f\"\\n==== Summary for {model_name} (over {num_runs} runs) ====\")\n",
    "        for metric in metrics_list:\n",
    "            mean_val = np.mean(metrics_list[metric])\n",
    "            std_val = np.std(metrics_list[metric])\n",
    "            print(f\"{metric.capitalize()}: {mean_val:.4f} ¬± {std_val:.4f}\")\n",
    "\n",
    "        # Plot boxplots of the 5 runs for metrics\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.boxplot(data=[metrics_list[\"accuracy\"], metrics_list[\"f1\"], metrics_list[\"auc\"]],\n",
    "                    palette=\"Set2\")\n",
    "        plt.xticks([0,1,2], ['Accuracy', 'F1-score', 'AUC'])\n",
    "        plt.title(f'{model_name} Metrics Boxplot over {num_runs} runs')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Save all results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    csv_save_path = os.path.join(save_dir, f\"results_all_models_{feature_model_name}.csv\")\n",
    "    df.to_csv(csv_save_path, index=False)\n",
    "    print(f\"\\nAll run results saved to: {csv_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, GRU, SimpleRNN, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# --- Data loading and preprocessing ---\n",
    "def prepare_data(normal_path, anomaly_path):\n",
    "    X_normal = np.load(normal_path, allow_pickle=True)\n",
    "    if X_normal.ndim >= 4:\n",
    "        X_normal = X_normal.mean(axis=(2, 3))\n",
    "    X_anomaly = np.load(anomaly_path, allow_pickle=True)\n",
    "    if X_anomaly.ndim >= 4:\n",
    "        X_anomaly = X_anomaly.mean(axis=(2, 3))\n",
    "    X = np.concatenate([X_normal, X_anomaly])\n",
    "    y = np.concatenate([np.zeros(len(X_normal)), np.ones(len(X_anomaly))])\n",
    "    X = X.astype(np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    return X, y\n",
    "\n",
    "def create_sequences(X, y, seq_len=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len + 1):\n",
    "        X_seq.append(X[i:i + seq_len])\n",
    "        y_seq.append(y[i + seq_len - 1])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# --- Model builders with 32 and 16 units ---\n",
    "def build_lstm_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, activation='tanh', recurrent_activation='sigmoid', unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GRU(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_lstm_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, return_sequences=True, unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = GRU(32)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_simple_rnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = SimpleRNN(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_dense_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Main execution ---\n",
    "def main():\n",
    "    feature_model_name = 'repvgg_a0'  # Change if needed\n",
    "    seq_len = 10\n",
    "    save_dir = '/content/drive/MyDrive/ped1'  # Change to your data path\n",
    "    normal_path = f\"{save_dir}/normal_features_{feature_model_name}.npy\"\n",
    "    anomaly_path = f\"{save_dir}/anomaly_features_{feature_model_name}.npy\"\n",
    "\n",
    "    X, y = prepare_data(normal_path, anomaly_path)\n",
    "    X_seq, y_seq = create_sequences(X, y, seq_len=seq_len)\n",
    "\n",
    "    model_builders = {\n",
    "        \"LSTM\": build_lstm_model,\n",
    "        \"GRU\": build_gru_model,\n",
    "        \"LSTM_GRU\": build_lstm_gru_model,\n",
    "        \"SimpleRNN\": build_simple_rnn_model,\n",
    "        \"Dense\": build_dense_model\n",
    "    }\n",
    "\n",
    "    num_runs = 5\n",
    "    all_results = []\n",
    "\n",
    "    for model_name, builder in model_builders.items():\n",
    "        print(f\"\\n==== Training model: {model_name} ====\")\n",
    "\n",
    "        metrics_list = {\n",
    "            \"accuracy\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": [],\n",
    "            \"auc\": []\n",
    "        }\n",
    "\n",
    "        histories = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            print(f\" Run {run+1}/{num_runs}\")\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_seq, y_seq, test_size=0.2, stratify=y_seq, random_state=run\n",
    "            )\n",
    "\n",
    "            model = builder(X_train.shape[1:])\n",
    "            es = EarlyStopping(patience=5, restore_best_weights=True, verbose=0)\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[es],\n",
    "                verbose=1\n",
    "            )\n",
    "            histories.append(history)\n",
    "\n",
    "            y_prob = model.predict(X_test).ravel()\n",
    "            y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "            # Store metrics\n",
    "            metrics_list[\"accuracy\"].append(acc)\n",
    "            metrics_list[\"precision\"].append(prec)\n",
    "            metrics_list[\"recall\"].append(rec)\n",
    "            metrics_list[\"f1\"].append(f1)\n",
    "            metrics_list[\"auc\"].append(auc_score)\n",
    "\n",
    "            all_results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Run\": run + 1,\n",
    "                \"Accuracy\": acc,\n",
    "                \"Precision\": prec,\n",
    "                \"Recall\": rec,\n",
    "                \"F1\": f1,\n",
    "                \"AUC\": auc_score\n",
    "            })\n",
    "\n",
    "            # Clear to save memory\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "        # Plot training curves for last run\n",
    "        last_history = histories[-1]\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(last_history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(last_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'{model_name} Accuracy per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(last_history.history['loss'], label='Train Loss')\n",
    "        plt.plot(last_history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{model_name} Loss per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print mean ¬± std summary for all runs\n",
    "        print(f\"\\n==== Summary for {model_name} (over {num_runs} runs) ====\")\n",
    "        for metric in metrics_list:\n",
    "            mean_val = np.mean(metrics_list[metric])\n",
    "            std_val = np.std(metrics_list[metric])\n",
    "            print(f\"{metric.capitalize()}: {mean_val:.4f} ¬± {std_val:.4f}\")\n",
    "\n",
    "        # Plot boxplots of the 5 runs for metrics\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.boxplot(data=[metrics_list[\"accuracy\"], metrics_list[\"f1\"], metrics_list[\"auc\"]],\n",
    "                    palette=\"Set2\")\n",
    "        plt.xticks([0,1,2], ['Accuracy', 'F1-score', 'AUC'])\n",
    "        plt.title(f'{model_name} Metrics Boxplot over {num_runs} runs')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Save all results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    csv_save_path = os.path.join(save_dir, f\"results_all_models_{feature_model_name}.csv\")\n",
    "    df.to_csv(csv_save_path, index=False)\n",
    "    print(f\"\\nAll run results saved to: {csv_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, GRU, SimpleRNN, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# --- Data loading and preprocessing ---\n",
    "def prepare_data(normal_path, anomaly_path):\n",
    "    X_normal = np.load(normal_path, allow_pickle=True)\n",
    "    if X_normal.ndim >= 4:\n",
    "        X_normal = X_normal.mean(axis=(2, 3))\n",
    "    X_anomaly = np.load(anomaly_path, allow_pickle=True)\n",
    "    if X_anomaly.ndim >= 4:\n",
    "        X_anomaly = X_anomaly.mean(axis=(2, 3))\n",
    "    X = np.concatenate([X_normal, X_anomaly])\n",
    "    y = np.concatenate([np.zeros(len(X_normal)), np.ones(len(X_anomaly))])\n",
    "    X = X.astype(np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    return X, y\n",
    "\n",
    "def create_sequences(X, y, seq_len=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len + 1):\n",
    "        X_seq.append(X[i:i + seq_len])\n",
    "        y_seq.append(y[i + seq_len - 1])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# --- Model builders with 32 and 16 units ---\n",
    "def build_lstm_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, activation='tanh', recurrent_activation='sigmoid', unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GRU(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_lstm_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, return_sequences=True, unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = GRU(32)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_simple_rnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = SimpleRNN(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_dense_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Main execution ---\n",
    "def main():\n",
    "    feature_model_name = 'mobileone_s0'  # Change if needed\n",
    "    seq_len = 10\n",
    "    save_dir = '/content/drive/MyDrive/ped1'  # Change to your data path\n",
    "    normal_path = f\"{save_dir}/normal_features_{feature_model_name}.npy\"\n",
    "    anomaly_path = f\"{save_dir}/anomaly_features_{feature_model_name}.npy\"\n",
    "\n",
    "    X, y = prepare_data(normal_path, anomaly_path)\n",
    "    X_seq, y_seq = create_sequences(X, y, seq_len=seq_len)\n",
    "\n",
    "    model_builders = {\n",
    "        \"LSTM\": build_lstm_model,\n",
    "        \"GRU\": build_gru_model,\n",
    "        \"LSTM_GRU\": build_lstm_gru_model,\n",
    "        \"SimpleRNN\": build_simple_rnn_model,\n",
    "        \"Dense\": build_dense_model\n",
    "    }\n",
    "\n",
    "    num_runs = 5\n",
    "    all_results = []\n",
    "\n",
    "    for model_name, builder in model_builders.items():\n",
    "        print(f\"\\n==== Training model: {model_name} ====\")\n",
    "\n",
    "        metrics_list = {\n",
    "            \"accuracy\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": [],\n",
    "            \"auc\": []\n",
    "        }\n",
    "\n",
    "        histories = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            print(f\" Run {run+1}/{num_runs}\")\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_seq, y_seq, test_size=0.2, stratify=y_seq, random_state=run\n",
    "            )\n",
    "\n",
    "            model = builder(X_train.shape[1:])\n",
    "            es = EarlyStopping(patience=5, restore_best_weights=True, verbose=0)\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[es],\n",
    "                verbose=1\n",
    "            )\n",
    "            histories.append(history)\n",
    "\n",
    "            y_prob = model.predict(X_test).ravel()\n",
    "            y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "            # Store metrics\n",
    "            metrics_list[\"accuracy\"].append(acc)\n",
    "            metrics_list[\"precision\"].append(prec)\n",
    "            metrics_list[\"recall\"].append(rec)\n",
    "            metrics_list[\"f1\"].append(f1)\n",
    "            metrics_list[\"auc\"].append(auc_score)\n",
    "\n",
    "            all_results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Run\": run + 1,\n",
    "                \"Accuracy\": acc,\n",
    "                \"Precision\": prec,\n",
    "                \"Recall\": rec,\n",
    "                \"F1\": f1,\n",
    "                \"AUC\": auc_score\n",
    "            })\n",
    "\n",
    "            # Clear to save memory\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "        # Plot training curves for last run\n",
    "        last_history = histories[-1]\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(last_history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(last_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'{model_name} Accuracy per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(last_history.history['loss'], label='Train Loss')\n",
    "        plt.plot(last_history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{model_name} Loss per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print mean ¬± std summary for all runs\n",
    "        print(f\"\\n==== Summary for {model_name} (over {num_runs} runs) ====\")\n",
    "        for metric in metrics_list:\n",
    "            mean_val = np.mean(metrics_list[metric])\n",
    "            std_val = np.std(metrics_list[metric])\n",
    "            print(f\"{metric.capitalize()}: {mean_val:.4f} ¬± {std_val:.4f}\")\n",
    "\n",
    "        # Plot boxplots of the 5 runs for metrics\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.boxplot(data=[metrics_list[\"accuracy\"], metrics_list[\"f1\"], metrics_list[\"auc\"]],\n",
    "                    palette=\"Set2\")\n",
    "        plt.xticks([0,1,2], ['Accuracy', 'F1-score', 'AUC'])\n",
    "        plt.title(f'{model_name} Metrics Boxplot over {num_runs} runs')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Save all results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    csv_save_path = os.path.join(save_dir, f\"results_all_models_{feature_model_name}.csv\")\n",
    "    df.to_csv(csv_save_path, index=False)\n",
    "    print(f\"\\nAll run results saved to: {csv_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, GRU, SimpleRNN, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# --- Data loading and preprocessing ---\n",
    "def prepare_data(normal_path, anomaly_path):\n",
    "    X_normal = np.load(normal_path, allow_pickle=True)\n",
    "    if X_normal.ndim >= 4:\n",
    "        X_normal = X_normal.mean(axis=(2, 3))\n",
    "    X_anomaly = np.load(anomaly_path, allow_pickle=True)\n",
    "    if X_anomaly.ndim >= 4:\n",
    "        X_anomaly = X_anomaly.mean(axis=(2, 3))\n",
    "    X = np.concatenate([X_normal, X_anomaly])\n",
    "    y = np.concatenate([np.zeros(len(X_normal)), np.ones(len(X_anomaly))])\n",
    "    X = X.astype(np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    return X, y\n",
    "\n",
    "def create_sequences(X, y, seq_len=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len + 1):\n",
    "        X_seq.append(X[i:i + seq_len])\n",
    "        y_seq.append(y[i + seq_len - 1])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# --- Model builders with 32 and 16 units ---\n",
    "def build_lstm_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, activation='tanh', recurrent_activation='sigmoid', unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GRU(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_lstm_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, return_sequences=True, unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = GRU(32)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_simple_rnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = SimpleRNN(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_dense_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Main execution ---\n",
    "def main():\n",
    "    feature_model_name = 'poolformer_s12'  # Change if needed\n",
    "    seq_len = 10\n",
    "    save_dir = '/content/drive/MyDrive/ped1'  # Change to your data path\n",
    "    normal_path = f\"{save_dir}/normal_features_{feature_model_name}.npy\"\n",
    "    anomaly_path = f\"{save_dir}/anomaly_features_{feature_model_name}.npy\"\n",
    "\n",
    "    X, y = prepare_data(normal_path, anomaly_path)\n",
    "    X_seq, y_seq = create_sequences(X, y, seq_len=seq_len)\n",
    "\n",
    "    model_builders = {\n",
    "        \"LSTM\": build_lstm_model,\n",
    "        \"GRU\": build_gru_model,\n",
    "        \"LSTM_GRU\": build_lstm_gru_model,\n",
    "        \"SimpleRNN\": build_simple_rnn_model,\n",
    "        \"Dense\": build_dense_model\n",
    "    }\n",
    "\n",
    "    num_runs = 5\n",
    "    all_results = []\n",
    "\n",
    "    for model_name, builder in model_builders.items():\n",
    "        print(f\"\\n==== Training model: {model_name} ====\")\n",
    "\n",
    "        metrics_list = {\n",
    "            \"accuracy\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": [],\n",
    "            \"auc\": []\n",
    "        }\n",
    "\n",
    "        histories = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            print(f\" Run {run+1}/{num_runs}\")\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_seq, y_seq, test_size=0.2, stratify=y_seq, random_state=run\n",
    "            )\n",
    "\n",
    "            model = builder(X_train.shape[1:])\n",
    "            es = EarlyStopping(patience=5, restore_best_weights=True, verbose=0)\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[es],\n",
    "                verbose=1\n",
    "            )\n",
    "            histories.append(history)\n",
    "\n",
    "            y_prob = model.predict(X_test).ravel()\n",
    "            y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "            # Store metrics\n",
    "            metrics_list[\"accuracy\"].append(acc)\n",
    "            metrics_list[\"precision\"].append(prec)\n",
    "            metrics_list[\"recall\"].append(rec)\n",
    "            metrics_list[\"f1\"].append(f1)\n",
    "            metrics_list[\"auc\"].append(auc_score)\n",
    "\n",
    "            all_results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Run\": run + 1,\n",
    "                \"Accuracy\": acc,\n",
    "                \"Precision\": prec,\n",
    "                \"Recall\": rec,\n",
    "                \"F1\": f1,\n",
    "                \"AUC\": auc_score\n",
    "            })\n",
    "\n",
    "            # Clear to save memory\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "        # Plot training curves for last run\n",
    "        last_history = histories[-1]\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(last_history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(last_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'{model_name} Accuracy per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(last_history.history['loss'], label='Train Loss')\n",
    "        plt.plot(last_history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{model_name} Loss per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print mean ¬± std summary for all runs\n",
    "        print(f\"\\n==== Summary for {model_name} (over {num_runs} runs) ====\")\n",
    "        for metric in metrics_list:\n",
    "            mean_val = np.mean(metrics_list[metric])\n",
    "            std_val = np.std(metrics_list[metric])\n",
    "            print(f\"{metric.capitalize()}: {mean_val:.4f} ¬± {std_val:.4f}\")\n",
    "\n",
    "        # Plot boxplots of the 5 runs for metrics\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.boxplot(data=[metrics_list[\"accuracy\"], metrics_list[\"f1\"], metrics_list[\"auc\"]],\n",
    "                    palette=\"Set2\")\n",
    "        plt.xticks([0,1,2], ['Accuracy', 'F1-score', 'AUC'])\n",
    "        plt.title(f'{model_name} Metrics Boxplot over {num_runs} runs')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Save all results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    csv_save_path = os.path.join(save_dir, f\"results_all_models_{feature_model_name}.csv\")\n",
    "    df.to_csv(csv_save_path, index=False)\n",
    "    print(f\"\\nAll run results saved to: {csv_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, GRU, SimpleRNN, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# --- Data loading and preprocessing ---\n",
    "def prepare_data(normal_path, anomaly_path):\n",
    "    X_normal = np.load(normal_path, allow_pickle=True)\n",
    "    if X_normal.ndim >= 4:\n",
    "        X_normal = X_normal.mean(axis=(2, 3))\n",
    "    X_anomaly = np.load(anomaly_path, allow_pickle=True)\n",
    "    if X_anomaly.ndim >= 4:\n",
    "        X_anomaly = X_anomaly.mean(axis=(2, 3))\n",
    "    X = np.concatenate([X_normal, X_anomaly])\n",
    "    y = np.concatenate([np.zeros(len(X_normal)), np.ones(len(X_anomaly))])\n",
    "    X = X.astype(np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    return X, y\n",
    "\n",
    "def create_sequences(X, y, seq_len=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len + 1):\n",
    "        X_seq.append(X[i:i + seq_len])\n",
    "        y_seq.append(y[i + seq_len - 1])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# --- Model builders with 32 and 16 units ---\n",
    "def build_lstm_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, activation='tanh', recurrent_activation='sigmoid', unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GRU(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_lstm_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, return_sequences=True, unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = GRU(32)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_simple_rnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = SimpleRNN(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_dense_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Main execution ---\n",
    "def main():\n",
    "    feature_model_name = 'maxvit_tiny_tf_224'  # Change if needed\n",
    "    seq_len = 10\n",
    "    save_dir = '/content/drive/MyDrive/ped1'  # Change to your data path\n",
    "    normal_path = f\"{save_dir}/normal_features_{feature_model_name}.npy\"\n",
    "    anomaly_path = f\"{save_dir}/anomaly_features_{feature_model_name}.npy\"\n",
    "\n",
    "    X, y = prepare_data(normal_path, anomaly_path)\n",
    "    X_seq, y_seq = create_sequences(X, y, seq_len=seq_len)\n",
    "\n",
    "    model_builders = {\n",
    "        \"LSTM\": build_lstm_model,\n",
    "        \"GRU\": build_gru_model,\n",
    "        \"LSTM_GRU\": build_lstm_gru_model,\n",
    "        \"SimpleRNN\": build_simple_rnn_model,\n",
    "        \"Dense\": build_dense_model\n",
    "    }\n",
    "\n",
    "    num_runs = 5\n",
    "    all_results = []\n",
    "\n",
    "    for model_name, builder in model_builders.items():\n",
    "        print(f\"\\n==== Training model: {model_name} ====\")\n",
    "\n",
    "        metrics_list = {\n",
    "            \"accuracy\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": [],\n",
    "            \"auc\": []\n",
    "        }\n",
    "\n",
    "        histories = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            print(f\" Run {run+1}/{num_runs}\")\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_seq, y_seq, test_size=0.2, stratify=y_seq, random_state=run\n",
    "            )\n",
    "\n",
    "            model = builder(X_train.shape[1:])\n",
    "            es = EarlyStopping(patience=5, restore_best_weights=True, verbose=0)\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[es],\n",
    "                verbose=1\n",
    "            )\n",
    "            histories.append(history)\n",
    "\n",
    "            y_prob = model.predict(X_test).ravel()\n",
    "            y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "            # Store metrics\n",
    "            metrics_list[\"accuracy\"].append(acc)\n",
    "            metrics_list[\"precision\"].append(prec)\n",
    "            metrics_list[\"recall\"].append(rec)\n",
    "            metrics_list[\"f1\"].append(f1)\n",
    "            metrics_list[\"auc\"].append(auc_score)\n",
    "\n",
    "            all_results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Run\": run + 1,\n",
    "                \"Accuracy\": acc,\n",
    "                \"Precision\": prec,\n",
    "                \"Recall\": rec,\n",
    "                \"F1\": f1,\n",
    "                \"AUC\": auc_score\n",
    "            })\n",
    "\n",
    "            # Clear to save memory\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "        # Plot training curves for last run\n",
    "        last_history = histories[-1]\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(last_history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(last_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'{model_name} Accuracy per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(last_history.history['loss'], label='Train Loss')\n",
    "        plt.plot(last_history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{model_name} Loss per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print mean ¬± std summary for all runs\n",
    "        print(f\"\\n==== Summary for {model_name} (over {num_runs} runs) ====\")\n",
    "        for metric in metrics_list:\n",
    "            mean_val = np.mean(metrics_list[metric])\n",
    "            std_val = np.std(metrics_list[metric])\n",
    "            print(f\"{metric.capitalize()}: {mean_val:.4f} ¬± {std_val:.4f}\")\n",
    "\n",
    "        # Plot boxplots of the 5 runs for metrics\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.boxplot(data=[metrics_list[\"accuracy\"], metrics_list[\"f1\"], metrics_list[\"auc\"]],\n",
    "                    palette=\"Set2\")\n",
    "        plt.xticks([0,1,2], ['Accuracy', 'F1-score', 'AUC'])\n",
    "        plt.title(f'{model_name} Metrics Boxplot over {num_runs} runs')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Save all results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    csv_save_path = os.path.join(save_dir, f\"results_all_models_{feature_model_name}.csv\")\n",
    "    df.to_csv(csv_save_path, index=False)\n",
    "    print(f\"\\nAll run results saved to: {csv_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, GRU, SimpleRNN, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# --- Data loading and preprocessing ---\n",
    "def prepare_data(normal_path, anomaly_path):\n",
    "    X_normal = np.load(normal_path, allow_pickle=True)\n",
    "    if X_normal.ndim >= 4:\n",
    "        X_normal = X_normal.mean(axis=(2, 3))\n",
    "    X_anomaly = np.load(anomaly_path, allow_pickle=True)\n",
    "    if X_anomaly.ndim >= 4:\n",
    "        X_anomaly = X_anomaly.mean(axis=(2, 3))\n",
    "    X = np.concatenate([X_normal, X_anomaly])\n",
    "    y = np.concatenate([np.zeros(len(X_normal)), np.ones(len(X_anomaly))])\n",
    "    X = X.astype(np.float32)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "    return X, y\n",
    "\n",
    "def create_sequences(X, y, seq_len=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_len + 1):\n",
    "        X_seq.append(X[i:i + seq_len])\n",
    "        y_seq.append(y[i + seq_len - 1])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# --- Model builders with 32 and 16 units ---\n",
    "def build_lstm_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, activation='tanh', recurrent_activation='sigmoid', unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = GRU(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_lstm_gru_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, return_sequences=True, unroll=True)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = GRU(32)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_simple_rnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = SimpleRNN(32)(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_dense_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# --- Main execution ---\n",
    "def main():\n",
    "    feature_model_name = 'coatnet_0_rw_224'  # Change if needed\n",
    "    seq_len = 10\n",
    "    save_dir = '/content/drive/MyDrive/ped1'  # Change to your data path\n",
    "    normal_path = f\"{save_dir}/normal_features_{feature_model_name}.npy\"\n",
    "    anomaly_path = f\"{save_dir}/anomaly_features_{feature_model_name}.npy\"\n",
    "\n",
    "    X, y = prepare_data(normal_path, anomaly_path)\n",
    "    X_seq, y_seq = create_sequences(X, y, seq_len=seq_len)\n",
    "\n",
    "    model_builders = {\n",
    "        \"LSTM\": build_lstm_model,\n",
    "        \"GRU\": build_gru_model,\n",
    "        \"LSTM_GRU\": build_lstm_gru_model,\n",
    "        \"SimpleRNN\": build_simple_rnn_model,\n",
    "        \"Dense\": build_dense_model\n",
    "    }\n",
    "\n",
    "    num_runs = 5\n",
    "    all_results = []\n",
    "\n",
    "    for model_name, builder in model_builders.items():\n",
    "        print(f\"\\n==== Training model: {model_name} ====\")\n",
    "\n",
    "        metrics_list = {\n",
    "            \"accuracy\": [],\n",
    "            \"precision\": [],\n",
    "            \"recall\": [],\n",
    "            \"f1\": [],\n",
    "            \"auc\": []\n",
    "        }\n",
    "\n",
    "        histories = []\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            print(f\" Run {run+1}/{num_runs}\")\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_seq, y_seq, test_size=0.2, stratify=y_seq, random_state=run\n",
    "            )\n",
    "\n",
    "            model = builder(X_train.shape[1:])\n",
    "            es = EarlyStopping(patience=5, restore_best_weights=True, verbose=0)\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[es],\n",
    "                verbose=1\n",
    "            )\n",
    "            histories.append(history)\n",
    "\n",
    "            y_prob = model.predict(X_test).ravel()\n",
    "            y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "            auc_score = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "            # Store metrics\n",
    "            metrics_list[\"accuracy\"].append(acc)\n",
    "            metrics_list[\"precision\"].append(prec)\n",
    "            metrics_list[\"recall\"].append(rec)\n",
    "            metrics_list[\"f1\"].append(f1)\n",
    "            metrics_list[\"auc\"].append(auc_score)\n",
    "\n",
    "            all_results.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Run\": run + 1,\n",
    "                \"Accuracy\": acc,\n",
    "                \"Precision\": prec,\n",
    "                \"Recall\": rec,\n",
    "                \"F1\": f1,\n",
    "                \"AUC\": auc_score\n",
    "            })\n",
    "\n",
    "            # Clear to save memory\n",
    "            K.clear_session()\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "        # Plot training curves for last run\n",
    "        last_history = histories[-1]\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(last_history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(last_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'{model_name} Accuracy per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(last_history.history['loss'], label='Train Loss')\n",
    "        plt.plot(last_history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{model_name} Loss per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print mean ¬± std summary for all runs\n",
    "        print(f\"\\n==== Summary for {model_name} (over {num_runs} runs) ====\")\n",
    "        for metric in metrics_list:\n",
    "            mean_val = np.mean(metrics_list[metric])\n",
    "            std_val = np.std(metrics_list[metric])\n",
    "            print(f\"{metric.capitalize()}: {mean_val:.4f} ¬± {std_val:.4f}\")\n",
    "\n",
    "        # Plot boxplots of the 5 runs for metrics\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.boxplot(data=[metrics_list[\"accuracy\"], metrics_list[\"f1\"], metrics_list[\"auc\"]],\n",
    "                    palette=\"Set2\")\n",
    "        plt.xticks([0,1,2], ['Accuracy', 'F1-score', 'AUC'])\n",
    "        plt.title(f'{model_name} Metrics Boxplot over {num_runs} runs')\n",
    "        plt.ylabel('Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Save all results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    csv_save_path = os.path.join(save_dir, f\"results_all_models_{feature_model_name}.csv\")\n",
    "    df.to_csv(csv_save_path, index=False)\n",
    "    print(f\"\\nAll run results saved to: {csv_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
